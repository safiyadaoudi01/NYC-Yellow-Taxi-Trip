{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d191aa-2e58-4fce-930b-355e94409cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importation des biblotheques\n",
    "from pyspark.sql.types import LongType, DoubleType, StringType, TimestampType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9629f85f-c407-4137-be22-6bf7fe99255a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lecture des fichiers parquet\n",
    "folder = \"/Volumes/workspace/trips/yellow_taxi\"\n",
    "files = dbutils.fs.ls(folder)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in files:\n",
    "    if f.path.endswith(\".parquet\"):\n",
    "        tmp = spark.read.parquet(f.path)\n",
    "        # Uniformiser les types et noms\n",
    "        tmp = tmp.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType())) \\\n",
    "                 .withColumn(\"passenger_count\", col(\"passenger_count\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType())) \\\n",
    "                 .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType())) \\\n",
    "                 .withColumnRenamed(\"Airport_fee\", \"airport_fee\")  # uniformiser majuscule\n",
    "        dfs.append(tmp)\n",
    "\n",
    "# Combiner tous les DataFrames\n",
    "\n",
    "df = reduce(DataFrame.unionByName, dfs)\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe6c10c-e582-4dfc-8c60-adb9c5960bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Réécrire tous les fichiers Parquet avec un schéma uniforme\n",
    "df.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/trips/yellow_taxi_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd78b3f-6355-48fc-8f81-0643454d6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "df = spark.read.parquet(\"/Volumes/workspace/trips/yellow_taxi_clean\")\n",
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC_Yellow_Taxi_Trip",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
